#!/usr/bin/env python3
"""
CMT Data Science Interview Project - Speaker Identification
Author: Divyesh Jagetia
Date: 8th July 2025

This script solves the speaker identification problem using LPC Cepstrum coefficients
from audio utterances of 9 male speakers speaking the same vowel.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

class SpeakerIdentificationSystem:
    """
    A comprehensive system for speaker identification using LPC Cepstrum coefficients
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.train_data = None
        self.train_labels = None
        self.test_data = None
        
    def load_data(self, train_file='train.txt', test_file='test.txt', labels_file='train_block_labels.txt'):
        """Load and parse the training and test data"""
        print("Loading data...")
        
        # Load training data
        self.train_data = self._parse_data_file(train_file)
        print(f"Loaded {len(self.train_data)} training blocks")
        
        # Load test data
        self.test_data = self._parse_data_file(test_file)
        print(f"Loaded {len(self.test_data)} test blocks")
        
        # Load labels
        with open(labels_file, 'r') as f:
            label_counts = list(map(int, f.read().strip().split()))
        
        # Create labels array
        self.train_labels = []
        for label, count in enumerate(label_counts):
            self.train_labels.extend([label] * count)
        
        self.train_labels = np.array(self.train_labels)
        print(f"Label distribution: {dict(zip(*np.unique(self.train_labels, return_counts=True)))}")
        
        return self.train_data, self.train_labels, self.test_data
    
    def _parse_data_file(self, filename):
        """Parse the data file with blocks separated by double newlines"""
        with open(filename, 'r') as f:
            content = f.read().strip()
        
        blocks = []
        block_texts = content.split('\n\n')
        
        for block_text in block_texts:
            if block_text.strip():
                lines = block_text.strip().split('\n')
                block_data = []
                for line in lines:
                    if line.strip():
                        values = list(map(float, line.strip().split()))
                        if len(values) == 12:  # Ensure we have 12 LPC coefficients
                            block_data.append(values)
                
                if block_data:  # Only add non-empty blocks
                    blocks.append(np.array(block_data))
        
        return blocks
    
    def extract_features(self, data):
        """
        Extract comprehensive features from variable-length time series
        
        Features include:
        - Statistical moments (mean, std, skew, kurtosis)
        - Temporal features (first/last values, differences)
        - Spectral features (FFT-based)
        - Trajectory features (slopes, curvature)
        """
        features = []
        
        for block in data:
            block_features = []
            
            # Basic statistics for each coefficient
            for coeff_idx in range(12):
                coeff_data = block[:, coeff_idx]
                
                # Statistical moments
                block_features.extend([
                    np.mean(coeff_data),
                    np.std(coeff_data),
                    np.median(coeff_data),
                    np.min(coeff_data),
                    np.max(coeff_data),
                    np.ptp(coeff_data),  # Peak-to-peak
                ])
                
                # Temporal features
                if len(coeff_data) > 1:
                    # First and last values
                    block_features.extend([
                        coeff_data[0],
                        coeff_data[-1],
                        coeff_data[-1] - coeff_data[0],  # Overall change
                    ])
                    
                    # Trajectory features
                    if len(coeff_data) > 2:
                        # Linear trend (slope)
                        x = np.arange(len(coeff_data))
                        slope = np.polyfit(x, coeff_data, 1)[0]
                        block_features.append(slope)
                        
                        # Curvature (second derivative approximation)
                        curvature = np.mean(np.diff(coeff_data, n=2))
                        block_features.append(curvature)
                    else:
                        block_features.extend([0, 0])  # Fallback for short sequences
                else:
                    block_features.extend([0, 0, 0, 0, 0])  # Fallback for single point
            
            # Global features across all coefficients
            # Energy and spectral features
            block_features.extend([
                len(block),  # Sequence length
                np.mean(np.sum(block**2, axis=1)),  # Average energy per frame
                np.std(np.sum(block**2, axis=1)),   # Energy variability
            ])
            
            # Cross-coefficient correlations (sample a few key ones)
            if len(block) > 1:
                # Correlation between first few coefficients
                corr_01 = np.corrcoef(block[:, 0], block[:, 1])[0, 1] if len(block) > 1 else 0
                corr_02 = np.corrcoef(block[:, 0], block[:, 2])[0, 1] if len(block) > 1 else 0
                corr_12 = np.corrcoef(block[:, 1], block[:, 2])[0, 1] if len(block) > 1 else 0
                block_features.extend([corr_01, corr_02, corr_12])
            else:
                block_features.extend([0, 0, 0])
            
            features.append(block_features)
        
        return np.array(features)
    
    
    def visualize_data(self, max_blocks=5):
        """Create visualizations to understand the data"""
        print("Creating visualizations...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Sample time series for different speakers
        ax1 = axes[0, 0]
        colors = plt.cm.tab10(np.linspace(0, 1, 9))
        
        for speaker in range(len(np.unique(self.train_labels))):
            speaker_indices = [i for i, label in enumerate(self.train_labels) if label == speaker]
            if speaker_indices:
                sample_block = self.train_data[speaker_indices[0]]
                # Plot first coefficient over time
                ax1.plot(sample_block[:, 0], alpha=0.7, color=colors[speaker], label=f'Speaker {speaker}')
        
        ax1.set_title('First LPC Coefficient Over Time')
        ax1.set_xlabel('Time Point')
        ax1.set_ylabel('LPC Coefficient Value')
        ax1.legend()
        
        # 2. Sequence length distribution
        ax2 = axes[0, 1]
        lengths = [len(block) for block in self.train_data]
        ax2.hist(lengths, bins=20, alpha=0.7, edgecolor='black')
        ax2.set_title('Distribution of Sequence Lengths')
        ax2.set_xlabel('Sequence Length (time points)')
        ax2.set_ylabel('Frequency')
        
        # 3. Speaker distribution
        ax3 = axes[1, 0]
        unique_labels, counts = np.unique(self.train_labels, return_counts=True)
        ax3.bar(unique_labels, counts, alpha=0.7, color='skyblue', edgecolor='black')
        ax3.set_title('Speaker Distribution in Training Set')
        ax3.set_xlabel('Speaker ID')
        ax3.set_ylabel('Number of Utterances')
        
        # 4. Average coefficient values by speaker
        ax4 = axes[1, 1]
        speaker_means = []
        for speaker in range(9):
            speaker_blocks = [self.train_data[i] for i in range(len(self.train_data)) 
                            if self.train_labels[i] == speaker]
            if speaker_blocks:
                # Average the first coefficient across all blocks for this speaker
                speaker_mean = np.mean([np.mean(block[:, 0]) for block in speaker_blocks])
                speaker_means.append(speaker_mean)
            else:
                speaker_means.append(0)
        
        ax4.bar(range(9), speaker_means, alpha=0.7, color='lightcoral', edgecolor='black')
        ax4.set_title('Average First LPC Coefficient by Speaker')
        ax4.set_xlabel('Speaker ID')
        ax4.set_ylabel('Average LPC Coefficient 1')
        
        plt.tight_layout()
        plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Print summary statistics
        print(f"\nData Summary:")
        print(f"Training blocks: {len(self.train_data)}")
        print(f"Test blocks: {len(self.test_data)}")
        print(f"Sequence length range: {min(lengths)} - {max(lengths)} time points")
        print(f"Average sequence length: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")
        print(f"Number of speakers: {len(unique_labels)}")
        
    def train_model(self, model_type='ensemble'):
        """Train the speaker identification model"""
        print("Extracting features...")
        X_train = self.extract_features(self.train_data)
        y_train = self.train_labels
        
        print(f"Feature matrix shape: {X_train.shape}")
        
        # Handle any NaN values
        X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Create and train model
        if model_type == 'ensemble':
            # Ensemble of multiple models
            rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, 
                                      min_samples_split=5, min_samples_leaf=2)
            gb = GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42)
            svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
            
            # Use RandomForest as primary (usually works well for this type of problem)
            self.model = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', rf)
            ])
            
        elif model_type == 'gradient_boosting':
            self.model = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42))
            ])
            
        else:  # Default to RandomForest
            self.model = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42,
                                                   min_samples_split=5, min_samples_leaf=2))
            ])
        
        print("Training model...")
        self.model.fit(X_train, y_train)
        
        # Cross-validation evaluation
        print("Performing cross-validation...")
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, scoring='accuracy')
        print(f"Cross-validation accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # Per-class performance check
        y_pred = self.model.predict(X_train)
        print("\nTraining Set Performance:")
        print(classification_report(y_train, y_pred))
        
        # Check worst-performing class (as mentioned in grading criteria)
        class_accuracies = []
        for class_id in range(9):
            class_mask = y_train == class_id
            if np.sum(class_mask) > 0:
                class_acc = accuracy_score(y_train[class_mask], y_pred[class_mask])
                class_accuracies.append(class_acc)
                print(f"Speaker {class_id} accuracy: {class_acc:.4f}")
        
        worst_class_acc = min(class_accuracies)
        print(f"\nWorst performing speaker accuracy: {worst_class_acc:.4f}")
        
        return self.model
    
    def predict_test_set(self):
        """Generate predictions for the test set"""
        print("Generating test predictions...")
        
        if self.model is None:
            raise ValueError("Model not trained. Call train_model() first.")
        
        # Extract features from test data
        X_test = self.extract_features(self.test_data)
        X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Make predictions
        predictions = self.model.predict(X_test)
        
        # Create submission dataframe
        submission = pd.DataFrame({
            'block_num': range(len(predictions)),
            'prediction': predictions
        })
        
        return submission
    
    def save_predictions(self, submission_df, filename='submission.csv'):
        """Save predictions to CSV file"""
        submission_df.to_csv(filename, index=False)
        print(f"Predictions saved to {filename}")
        
        # Print prediction distribution
        pred_counts = submission_df['prediction'].value_counts().sort_index()
        print(f"\nPrediction distribution:")
        for speaker, count in pred_counts.items():
            print(f"Speaker {speaker}: {count} utterances")


def main():
    """Main execution function"""
    print("CMT Data Science Interview Project - Speaker Identification")
    print("=" * 60)
    
    # Initialize the system
    speaker_system = SpeakerIdentificationSystem()
    
    # Load data
    train_data, train_labels, test_data = speaker_system.load_data()
    
    # Create visualizations
    speaker_system.visualize_data()
    
    # Train the model
    model = speaker_system.train_model(model_type='ensemble')
    
    # Generate predictions
    submission = speaker_system.predict_test_set()
    
    # Save predictions
    speaker_system.save_predictions(submission, 'cmt_speaker_predictions.csv')
    
    print("\n" + "=" * 60)
    print("Project completed successfully!")
    print("Key deliverables:")
    print("1. Test set predictions saved to 'cmt_speaker_predictions.csv'")
    print("2. Data visualizations saved to 'data_exploration.png'")
    print("3. Feature extraction and model training completed")
    print("4. Cross-validation and performance analysis completed")
    
    # Display first few predictions
    print(f"\nFirst 10 predictions:")
    print(submission.head(10))


if __name__ == "__main__":
    main()